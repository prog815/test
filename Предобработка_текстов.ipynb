{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prog815/test/blob/master/%D0%9F%D1%80%D0%B5%D0%B4%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcL7r1hqySq"
      },
      "source": [
        "## Предобработка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlTJp0h2q9px"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn8EWAjnr18g"
      },
      "source": [
        "### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btBdBLxbgrNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82bcccf3-6fae-471d-ce6a-df3c7ac523b5"
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx3uBJlfsIQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af250510-f296-42d8-cbc1-dba3453db81a"
      },
      "source": [
        "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
        "tokens = word_tokenize(data.lower())\n",
        "print(tokens)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrl8vQD2E8G0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da47bcf-e34c-432b-b0e3-bdcca893def8"
      },
      "source": [
        "text = \"Кому-то повезло, кому-то нет. I don't know. Миру - мир!\"\n",
        "print(word_tokenize(text.lower()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['кому-то', 'повезло', ',', 'кому-то', 'нет', '.', 'i', 'do', \"n't\", 'know', '.', 'миру', '-', 'мир', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLHzXT6fE2Hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a361efb-1028-4be2-e320-960e6ef8dc9f"
      },
      "source": [
        "text = \"Ай да А.С. Пушкин! Ай да сукин сын!\"\n",
        "print(\"Before:\", nltk.sent_tokenize(text))\n",
        "print(\"After:\", nltk.sent_tokenize(text, language=\"russian\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: ['Ай да А.С.', 'Пушкин!', 'Ай да сукин сын!']\n",
            "After: ['Ай да А.С. Пушкин!', 'Ай да сукин сын!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXsSXmPPY7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10648f98-5b28-4605-e264-260f148900d1"
      },
      "source": [
        "text = \"@Volk написал Твит. :-) #token\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'Volk', 'написал', 'Твит', '.', ':', '-', ')', '#', 'token']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33V93DMyQCg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b6995f-d786-4b5c-bcb1-c652a514f22e"
      },
      "source": [
        "from nltk.tokenize.casual import casual_tokenize\n",
        "print(casual_tokenize(text))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@Volk', 'написал', 'Твит', '.', ':-)', '#token']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X1eleVAJ0bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a42186f-1563-4730-c632-e41fcc11c88d"
      },
      "source": [
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "t = TweetTokenizer()\n",
        "t.tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@Volk', 'написал', 'Твит', '.', ':-)', '#token']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trdPOBM2jEMf"
      },
      "source": [
        "#### N-граммы\n",
        "\n",
        "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--466CQV1q--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/78nf1vryed8h1tz05fim.gif\" height=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYEBfCxLic3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96a9eeb-8c52-4dcd-d1ec-aafdfa25c807"
      },
      "source": [
        "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
        "tokens = word_tokenize(data.lower())\n",
        "unigram = list(nltk.ngrams(tokens, 1))\n",
        "bigram = list(nltk.ngrams(tokens, 2))\n",
        "print(tokens)\n",
        "print(unigram[:5])\n",
        "print(bigram[:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n",
            "[('all',), ('work',), ('and',), ('no',), ('play',)]\n",
            "[('all', 'work'), ('work', 'and'), ('and', 'no'), ('no', 'play'), ('play', 'makes')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AFeZqejmWwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f2e3f-aa61-4d72-dac7-4fcd7cc442ef"
      },
      "source": [
        "from nltk import FreqDist\n",
        "print('Популярные униграммы: ', FreqDist(unigram).most_common(5))\n",
        "print('Популярные биграммы: ', FreqDist(bigram).most_common(5))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Популярные униграммы:  [(('all',), 2), (('work',), 2), (('and',), 2), (('no',), 2), (('play',), 2)]\n",
            "Популярные биграммы:  [(('all', 'work'), 2), (('work', 'and'), 2), (('and', 'no'), 2), (('no', 'play'), 2), (('play', 'makes'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgKUR2LUMF_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952fb3a8-2b01-41c0-a0d0-4dd8ae1e4422"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1yNWwjKo_LeumeHtA60NhhyZU8Af58QQu"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yNWwjKo_LeumeHtA60NhhyZU8Af58QQu\n",
            "To: /content/59495692.txt\n",
            "\r  0% 0.00/894k [00:00<?, ?B/s]\r100% 894k/894k [00:00<00:00, 131MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNWiOThPERVM",
        "outputId": "94914b97-5117-459b-a22f-61612ba41024"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwtszG9WOCJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "b02e68fe-26ee-47eb-85fc-1c906cd0a394"
      },
      "source": [
        "f = open(\"/content/59495692.txt\")\n",
        "vm = f.read()\n",
        "vm[:1000]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Война и мир. Том I–II\\nЛев Николаевич Толстой\\n\\n\\nБиблиотека всемирной литературы (Эксмо)\\n«Война и мир»\\xa0– вершина творчества Л.Н. Толстого, как никакое другое произведение писателя отражает глубину его мироощущения и философии. Эта книга из разряда вечных, потому что она обо всем: о\\xa0жизни и смерти, о любви и чести, о мужестве и героизме, о славе и подвиге, о войне и мире. Самый известный во всем мире роман гениального писателя вот уже третье столетие заставляет читателей сопереживать героям произведения. Роман о русской душе, о русском укладе жизни, о вечных вопросах, которые приходится решать каждому человеку наедине с собой. Все жизненные перипетии героев, происходящие на фоне исторических событий, произошедших в начале ХIХ века с Россией, на фоне кровавых событий войны 1812 года обретают емкий философский смысл. Роман по глубине и охвату событий до сих пор стоит на первом месте во всей мировой литературе.\\n\\n\\n\\n\\n\\nЛев Николаевич Толстой\\n\\nВойна и мир\\n\\nТом I–II\\n\\n\\n\\n© Ранчин А.М., предисловие,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ltJK6s6R04L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba38cd0-6b72-41fb-ab60-23338e76d046"
      },
      "source": [
        "vmtokens = word_tokenize(vm.lower())\n",
        "unigram = list(nltk.ngrams(vmtokens, 1))\n",
        "print('Популярные униграммы: ', FreqDist(unigram).most_common(15))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Популярные униграммы:  [((',',), 10624), (('.',), 5006), (('и',), 3252), (('–',), 3029), (('в',), 1655), (('не',), 1336), (('что',), 1057), (('на',), 1022), (('он',), 971), (('с',), 953), (('?',), 869), (('как',), 693), (('я',), 663), (('!',), 615), (('к',), 547)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = list(nltk.ngrams(vmtokens, 2))\n",
        "print('Популярные триграммы: ', FreqDist(bigram).most_common(15))"
      ],
      "metadata": {
        "id": "rq3l9oeqh--Y",
        "outputId": "e1a388b1-aae8-4edf-ad84-a9033d9db682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Популярные триграммы:  [(('.', '–'), 1315), ((',', '–'), 917), ((',', 'что'), 753), ((',', 'и'), 546), ((',', 'как'), 424), (('–', 'сказал'), 298), (('?', '–'), 229), ((',', 'не'), 219), ((',', 'но'), 205), ((',', 'а'), 187), (('князь', 'андрей'), 182), ((',', 'в'), 173), (('!', '–'), 172), ((',', 'с'), 155), (('.', 'он'), 154)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = list(nltk.ngrams(vmtokens, 3))\n",
        "print('Популярные триграммы: ', FreqDist(bigram).most_common(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckZkKK8PEh0Z",
        "outputId": "ac90a8cc-d5af-4e48-fb54-e580a455974c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Популярные триграммы:  [((',', '–', 'сказал'), 227), (('.', ']', ','), 113), (('–', 'сказал', 'он'), 102), ((',', '–', 'сказала'), 94), ((',', 'как', 'будто'), 83), (('.', '–', 'я'), 80), ((']', ',', '–'), 73), (('–', 'ну', ','), 71), (('.', '–', 'ну'), 60), (('.', 'князь', 'андрей'), 60), (('–', 'сказала', 'она'), 58), ((',', 'что', 'он'), 57), (('.', '–', 'да'), 51), (('–', 'сказал', 'князь'), 50), (('то', ',', 'что'), 47)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw69rmxFOBCi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W3jJ56hnBFu"
      },
      "source": [
        "#### Стоп-слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIBwQ3nBnEfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553d7cc7-da67-4fe1-e02b-0b54aa77e0fb"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1nk-TqEslRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81607057-bf48-4acb-f12b-5b820a3c997d"
      },
      "source": [
        "stopWords = set(stopwords.words('english'))\n",
        "print(len(stopWords))\n",
        "print(stopWords)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "{'with', 'ourselves', 'into', 'same', 'hers', 'can', 'some', 'other', 'out', 't', 'me', 'who', 'until', 'and', 'against', \"mustn't\", 'there', 'wouldn', 'the', 'on', 're', 'were', 've', 'all', 'has', 'after', 'themselves', 'll', \"mightn't\", 'about', 'few', 'or', 'herself', 'itself', \"hadn't\", 'when', 'to', 'for', 'as', 'from', 'here', 'why', 'any', 'a', 'each', 'needn', 'those', \"you'll\", 'yourself', \"didn't\", 'at', 'ain', 'have', 'was', 'an', 'he', 'before', 'very', 'these', 'yours', \"you're\", 'which', 'doesn', 'such', \"hasn't\", 'then', 'them', 'what', \"she's\", 'is', 'couldn', 'because', 'am', 'most', 'own', \"you'd\", 'now', 'above', \"it's\", 'again', 'where', 'd', 'over', 'haven', 'don', 'how', 'had', 'no', \"weren't\", 'more', 'whom', 'your', 'in', 'my', 'nor', 'so', \"you've\", 'yourselves', \"wasn't\", 'our', 'their', 'being', \"don't\", 'mightn', 'wasn', 'its', 'further', \"isn't\", 'shan', 'ma', 'didn', 'not', \"needn't\", 'theirs', 'i', 'does', 'having', 'o', 'will', \"won't\", 'are', 'be', 'y', \"should've\", 'myself', 'hasn', \"haven't\", \"shouldn't\", 'under', 'down', 'than', 'off', \"aren't\", 'if', 'only', 'but', 'through', \"couldn't\", \"that'll\", \"doesn't\", 'shouldn', 'between', 'both', 'we', 'aren', 'while', 'his', 'below', 'by', 'just', 'did', 'hadn', 'too', 'm', 'this', 'should', \"shan't\", 'that', 'once', 'him', 'won', \"wouldn't\", 'up', 'she', 'himself', 'ours', 'do', 'isn', 'of', 'during', 'her', 'you', 'doing', 'mustn', 'it', 'they', 's', 'been', 'weren'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yGCDgvrGPjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc08fe94-d98e-4f8a-e770-812f3d159f20"
      },
      "source": [
        "print(stopwords.fileids())\n",
        "stopWords_russian = set(stopwords.words('russian'))\n",
        "print(len(stopWords_russian))\n",
        "print(stopWords_russian)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
            "151\n",
            "{'много', 'нибудь', 'она', 'после', 'ну', 'им', 'ним', 'свою', 'вы', 'потому', 'сам', 'более', 'а', 'ли', 'тогда', 'какой', 'себе', 'того', 'то', 'зачем', 'без', 'меня', 'об', 'можно', 'разве', 'будет', 'над', 'с', 'за', 'нельзя', 'лучше', 'между', 'чего', 'как', 'и', 'нет', 'этот', 'когда', 'мой', 'больше', 'там', 'всех', 'моя', 'другой', 'почти', 'был', 'чтоб', 'наконец', 'бы', 'ж', 'этого', 'хорошо', 'уже', 'ни', 'его', 'через', 'такой', 'если', 'так', 'он', 'но', 'чтобы', 'мне', 'эти', 'вас', 'до', 'конечно', 'ей', 'чем', 'по', 'вот', 'хоть', 'тоже', 'здесь', 'ее', 'не', 'их', 'ему', 'о', 'со', 'все', 'из', 'нас', 'к', 'на', 'иногда', 'от', 'я', 'ней', 'совсем', 'них', 'опять', 'сейчас', 'всего', 'перед', 'всегда', 'нее', 'кто', 'при', 'быть', 'будто', 'да', 'чуть', 'ведь', 'под', 'есть', 'него', 'тут', 'том', 'тебя', 'тот', 'вам', 'надо', 'было', 'что', 'может', 'эту', 'ты', 'у', 'какая', 'теперь', 'этой', 'себя', 'три', 'была', 'даже', 'ничего', 'никогда', 'в', 'где', 'мы', 'два', 'еще', 'только', 'потом', 'про', 'для', 'один', 'они', 'же', 'впрочем', 'этом', 'уж', 'во', 'были', 'всю', 'раз', 'вдруг', 'куда', 'тем', 'или'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFkfJm9ktAVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ae8950-bece-41ff-ea1d-3717914910ca"
      },
      "source": [
        "from string import punctuation\n",
        "# отбираем слова(токены) исключая стоп-слова\n",
        "newvmtokens = [word for word in vmtokens if word not in  stopWords_russian | set(punctuation + '–«»“”')]\n",
        "# отбираем униграммы из текста\n",
        "unigram = list(nltk.ngrams(newvmtokens, 1))\n",
        "# выводим популярные униграммы с частотами\n",
        "print('Популярные униграммы: ', FreqDist(unigram).most_common(15))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Популярные униграммы:  [(('это',), 405), (('князь',), 376), (('сказал',), 371), (('андрей',), 201), (('de',), 179), (('пьер',), 178), (('сказала',), 155), (('очень',), 148), (('vous',), 136), (('анна',), 131), (('que',), 112), (('et',), 111), (('говорил',), 105), (('который',), 102), (('le',), 99)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# показываем состав массива пунктуации\n",
        "punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-OicRup_FZ-g",
        "outputId": "66578f19-8d42-4783-ecb4-3cee010f25ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-uKipvJRKoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38e0f9d-3b47-40cd-b32b-ca33e90c8396"
      },
      "source": [
        "# еще одна библиотека со стоп-словами\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "print(len(ENGLISH_STOP_WORDS))\n",
        "print(ENGLISH_STOP_WORDS)\n",
        "print(stopWords - ENGLISH_STOP_WORDS)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "318\n",
            "frozenset({'with', 'somehow', 'forty', 'behind', 'cry', 'give', 'another', 'all', 'after', 'etc', 'or', 'thru', 'eg', 'amoungst', 'five', 'as', 'nine', 'anyway', 'whole', 'thus', 'was', 'detail', 'latterly', 'he', 'very', 'then', 'eleven', 'mostly', 'per', 'sometime', 'without', 'even', 'co', 'alone', 'where', 'three', 'third', 'in', 'couldnt', 'show', 'my', 'so', 'everything', 'its', 'meanwhile', 'serious', 'due', 'fire', 'perhaps', 'whenever', 'than', 'off', 'only', 'but', 'system', 'thereupon', 'also', 'toward', 'although', 'by', 'except', 'may', 'first', 'amount', 'bottom', 'up', 'she', 'someone', 'you', 'moreover', 'they', 'same', 'can', 'out', 'until', 'ltd', 'nobody', 'on', 'nevertheless', 'were', 'however', 'interest', 'few', 'must', 'whereupon', 'top', 'to', 'becoming', 'any', 'former', 'seem', 'at', 'de', 'seeming', 'thick', 'within', 'whoever', 'therefore', 'become', 'hereby', 'became', 'please', 'is', 'least', 'either', 'am', 'now', 'above', 'un', 'whom', 'four', 'across', 'eight', 'being', 'hence', 'nothing', 'empty', 'further', 'formerly', 'via', 'one', 'would', 'i', 'are', 'be', 'myself', 'call', 'twelve', 'wherever', 'under', 'fill', 'put', 'whence', 'amongst', 'yet', 'below', 'his', 'whatever', 'name', 'besides', 'next', 'himself', 'two', 'part', 'during', 'find', 'it', 'each', 'sincere', 'describe', 'latter', 'some', 'nowhere', 'and', 'found', 'against', 'afterwards', 'there', 'con', 'the', 'mine', 'anyhow', 'fifteen', 'themselves', 'done', 'about', 'thin', 'get', 'herself', 'itself', 'whither', 'when', 'why', 'anyone', 'those', 'still', 'might', 'none', 'made', 'whereas', 'these', 'yours', 'herein', 'ie', 'twenty', 'others', 'such', 'what', 'upon', 'because', 'though', 'over', 'indeed', 'how', 'more', 'nor', 'their', 'every', 'less', 'see', 'not', 'back', 'since', 'never', 'ever', 'us', 'inc', 'somewhere', 'beyond', 'mill', 'sixty', 'fifty', 'both', 'cannot', 'towards', 'we', 'noone', 'else', 'while', 'side', 'along', 'should', 'that', 'thereafter', 'elsewhere', 'everyone', 'seemed', 'whereby', 'her', 'together', 'ourselves', 'into', 'hers', 'beforehand', 'around', 'other', 'wherein', 'me', 'who', 'everywhere', 're', 'has', 'move', 'last', 'always', 'anywhere', 'something', 'for', 'from', 'full', 'here', 'a', 'beside', 'thence', 'yourself', 'have', 'an', 'namely', 'before', 'which', 'hereafter', 'them', 'therein', 'most', 'thereby', 'own', 'again', 'go', 'hundred', 'whereafter', 'had', 'no', 'bill', 'your', 'front', 'yourselves', 'our', 'onto', 'already', 'throughout', 'ten', 'becomes', 'much', 'neither', 'will', 'cant', 'often', 'could', 'whether', 'several', 'down', 'sometimes', 'if', 'keep', 'rather', 'anything', 'through', 'between', 'take', 'otherwise', 'hasnt', 'among', 'well', 'too', 'enough', 'this', 'six', 'seems', 'many', 'once', 'him', 'almost', 'whose', 'hereupon', 'ours', 'do', 'of', 'been'})\n",
            "{'needn', \"you've\", \"wasn't\", \"don't\", 'mightn', \"you'll\", 'aren', 'wasn', 't', \"didn't\", 'ain', \"isn't\", 'shan', 'ma', 'didn', 'just', \"needn't\", 'did', 'hadn', \"you're\", 's', 'm', \"mustn't\", 'doesn', 'theirs', 'wouldn', \"shan't\", 'does', 'having', \"hasn't\", 'o', 've', \"she's\", 'couldn', \"won't\", \"doesn't\", 'y', 'won', \"should've\", \"you'd\", 'll', \"it's\", \"mightn't\", 'hasn', \"haven't\", \"shouldn't\", 'd', 'isn', 'haven', \"hadn't\", 'don', \"aren't\", 'doing', 'mustn', \"couldn't\", \"that'll\", \"weren't\", 'shouldn', \"wouldn't\", 'weren'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-x_dUtUGykQ",
        "outputId": "a2b6c37b-9f1d-4970-9cfd-160cf7fad278"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32896 sha256=513e1b882d51a0c68f734b92f43dbcb08094d4ca82106a46901eddf986170f67\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stop_words import get_stop_words\n",
        "len(get_stop_words('ru'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEM-KqjBG19X",
        "outputId": "7858677b-7661-493b-a76b-650f3ef9eb19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "421"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUKc1oTiQjf"
      },
      "source": [
        "### Стемминг\n",
        "* процесс нахождения основы слова для заданного исходного слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRVu-TrON4sq"
      },
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "words = [\"game\", \"gaming\", \"gamed\", \"games\", \"compacted\"]\n",
        "words_ru = ['корова', 'мальчики', 'мужчины', 'столом', 'убежала', 'бежать']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9HTGfsBN9eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2528498-b113-4d6e-a8e1-3c67f4ab1228"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "list(map(ps.stem, words))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['game', 'game', 'game', 'game', 'compact']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5qZkB-oODkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7025405c-e01c-44fb-a999-eae613bd57f7"
      },
      "source": [
        "ss = SnowballStemmer(language='russian')\n",
        "list(map(ss.stem, words_ru))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['коров', 'мальчик', 'мужчин', 'стол', 'убежа', 'бежа']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbTXbi9FJXr1"
      },
      "source": [
        "### Лематизация\n",
        "* процесс приведения словоформы к лемме — её нормальной (словарной) форме"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF4nnEz00thb"
      },
      "source": [
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "is no basis for a system of government.  Supreme executive power derives from\n",
        "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7MDqIvWib4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de41826-bdee-49cf-a9f7-04f4f63a510b"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(raw)\n",
        "print(' '.join([token.lemma_ for token in doc]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DENNIS : listen , strange woman lie in pond distribute sword \n",
            " be no basis for a system of government .   Supreme executive power derive from \n",
            " a mandate from the masse , not from some farcical aquatic ceremony .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doa85yw6JWea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c89cfe-9b80-4ebb-df27-e905fdafe82c"
      },
      "source": [
        "[(token.lemma_, token.pos_) for token in doc[:20]]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DENNIS', 'PROPN'),\n",
              " (':', 'PUNCT'),\n",
              " ('listen', 'VERB'),\n",
              " (',', 'PUNCT'),\n",
              " ('strange', 'ADJ'),\n",
              " ('woman', 'NOUN'),\n",
              " ('lie', 'VERB'),\n",
              " ('in', 'ADP'),\n",
              " ('pond', 'NOUN'),\n",
              " ('distribute', 'VERB'),\n",
              " ('sword', 'NOUN'),\n",
              " ('\\n', 'SPACE'),\n",
              " ('be', 'AUX'),\n",
              " ('no', 'DET'),\n",
              " ('basis', 'NOUN'),\n",
              " ('for', 'ADP'),\n",
              " ('a', 'DET'),\n",
              " ('system', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('government', 'NOUN')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RiBKluROwcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecd2095-b771-48d4-b988-02b10f3c1eae"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=f1b31f698dfe365bad3ee5388a1917d8f3a4957e0ea1790d8c8a8ff4d7fb251c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5c9t7TvOMkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d3f4d3-10d9-4445-bae3-e345e2fbcb31"
      },
      "source": [
        "from spacy.lang.ru import Russian, STOP_WORDS\n",
        "print(len(STOP_WORDS), STOP_WORDS) #to do ->stopwords\n",
        "\n",
        "nlp = Russian()\n",
        "\n",
        "sample_sentences = \"Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.\"\n",
        "\n",
        "doc = nlp(sample_sentences)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.norm_, token.is_alpha, token.is_stop, token.is_punct)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768 {'какого', 'кем', 'яко', 'других', 'некем', 'нибудь', 'недалеко', 'она', 'самое', 'им', 'ку', 'л', 'настоящая', 'поелику', 'свою', 'вниз', 'также', 'ах', 'иначе', 'посему', 'всём', 'таки', 'казалась', 'ещё', 'наверху', 'самих', 'нужных', 'вряд', 'нам', 'мной', 'того', 'ваши', 'нате', 'пор', 'навряд', 'которому', 'над', 'короче', 'с', 'своего', 'вопреки', 'ничему', 'прочему', 'всему', 'бытует', 'внакладе', 'начале', 'такие', 'такую', 'как', 'нет', 'наиболее', 'иль', 'давным', 'сродни', 'мою', 'одного', 'данунах', 'подавно', 'чтоб', 'наконец', 'кой', 'моих', 'сначала', 'своей', 'навыворот', 'через', 'ых', 'вресноту', 'али', 'мною', 'наши', 'затем', 'так', 'одни', 'всякого', 'таков', 'чему', 'вся', 'нынче', 'вправду', 'буду', 'всюду', 'нашем', 'прочее', 'должный', 'ей', 'сих', 'каждая', 'оне', 'чем', 'мочь', 'какие', 'здесь', 'своя', 'необходимости', 'другое', 'их', 'которых', 'которую', 'одними', 'е', 'находиться', 'около', 'о', 'меньше', 'со', 'все', 'з', 'стала', 'эта', 'доколь', 'нас', 'наподобие', 'другими', 'настоящие', 'на', 'от', 'ней', 'назад', 'будьте', 'которым', 'напротив', 'напрямую', 'оттот', 'включая', 'скорее', 'опять', 'особо', 'многие', 'любом', 'та', 'наоборот', 'да', 'своим', 'агу', 'общую', 'чуть', 'ведь', 'есть', 'оттуда', 'могут', 'сквозь', 'щас', 'было', 'вначале', 'даром', 'нею', 'г', 'ком', 'наше', 'сколько', 'следует', 'нашу', 'твоих', 'теперь', 'должно', 'данных', 'чём', 'любое', 'ранее', 'значит', 'была', 'даже', 'нужные', 'бывают', 'вправе', 'нем', 'данное', 'никогда', 'ка', 'бывает', 'где', 'накануне', 'ом', 'п', 'вокруг', 'никакой', 'только', 'поистине', 'особенно', 'ща', 'далеко', 'для', 'спустя', 'гораздо', 'мог', 'очень', 'слишком', 'во', 'были', 'этом', 'менее', 'всю', 'данные', 'тою', 'эх', 'твоё', 'едва', 'какому', 'такова', 'тем', 'ишь', 'необходимо', 'будешь', 'всякой', 'которая', 'нечто', 'явно', 'этими', 'оно', 'этакий', 'ним', 'нашим', 'одно', 'э', 'авось', 'своему', 'тогда', 'сызнова', 'пожалуй', 'оп', 'хуже', 'собою', 'зачем', 'без', 'едят', 'меня', 'особую', 'бывали', 'тому', 'моему', 'будете', 'нашему', 'близко', 'между', 'чего', 'прочими', 'нему', 'стать', 'вдали', 'этих', 'могли', 'экий', 'сей', 'одних', 'другой', 'почти', 'спокону', 'одною', 'самый', 'другие', 'комья', 'будь', 'её', 'бы', 'алло', 'этого', 'любого', 'данному', 'некто', 'свои', 'оба', 'подобного', 'ого', 'коли', 'насчет', 'если', 'мои', 'твоим', 'лишь', 'низко', 'который', 'таком', 'отовсюду', 'другим', 'вон', 'данного', 'будучи', 'неоткуда', 'до', 'конечно', 'сие', 'покудова', 'весь', 'по', 'можете', 'хоть', 'такому', 'точно', 'кажется', 'прежде', 'самом', 'можем', 'бываю', 'должны', 'ничем', 'моги', 'немного', 'неужели', 'взаимно', 'тотчас', 'к', 'которыми', 'всеми', 'просто', 'совсем', 'тобой', 'некуда', 'тьфу', 'хотел', 'я', 'кому', 'обычно', 'своём', 'притом', 'ха', 'стало', 'самим', 'нечего', 'сейчас', 'немногим', 'перед', 'ти', 'дану', 'ую', 'кто', 'многому', 'наизворот', 'моего', 'данной', 'доколе', 'всё', 'довольно', 'смогут', 'моём', 'благодаря', 'д', 'паче', 'казались', 'мало', 'стали', 'тех', 'том', 'туда', 'увы', 'тебя', 'тот', 'поэтому', 'любая', 'таких', 'этим', 'некоторых', 'вчеред', 'касательно', 'эка', 'самому', 'ай', 'хе', 'нету', 'которое', 'покуда', 'каков', 'ныне', 'какая', 'себя', 'снова', 'неважно', 'нешто', 'ел', 'ко', 'ага', 'бишь', 'хотя', 'подобная', 'в', 'возможно', 'му', 'наш', 'ох', 'еще', 'наизнанку', 'дополнительно', 'позже', 'какими', 'ближайшие', 'каждые', 'далее', 'же', 'казалось', 'уж', 'эдак', 'явных', 'бывала', 'кстати', 'вдруг', 'поколь', 'такими', 'подобно', 'сама', 'или', 'наша', 'много', 'собой', 'щ', 'недавно', 'едим', 'ими', 'наперед', 'после', 'многом', 'ну', 'нужного', 'одной', 'видно', 'имела', 'такого', 'кроме', 'отчему', 'отнелиже', 'вы', 'потому', 'а', 'ли', 'должен', 'эдакий', 'потомушта', 'среди', 'немногие', 'то', 'везде', 'можно', 'разве', 'будет', 'самими', 'иметь', 'никому', 'сперва', 'должна', 'нами', 'и', 'нашими', 'подобный', 'данном', 'когда', 'тебе', 'ваш', 'моей', 'этот', 'там', 'которой', 'тая', 'непрерывно', 'впрямь', 'нужно', 'любой', 'какою', 'которою', 'прочего', 'всякий', 'ж', 'хорошо', 'уже', 'нередко', 'вдобавок', 'одном', 'такой', 'поколе', 'самая', 'вне', 'своею', 'настоящее', 'ем', 'суть', 'всем', 'чтобы', 'хотеть', 'каким', 'вас', 'сразу', 'этою', 'дабы', 'вот', 'пора', 'свое', 'тоже', 'сможет', 'ее', 'которого', 'не', 'особые', 'посредством', 'одним', 'поприще', 'нипочем', 'отчего', 'ешь', 'якобы', 'собственно', 'взаправду', 'мол', 'вообще', 'ша', 'из', 'одна', 'очевидно', 'пожалуйста', 'иногда', 'своем', 'вовсе', 'чё', 'хотелось', 'например', 'ту', 'неё', 'нужная', 'оттого', 'х', 'всегда', 'при', 'настоящий', 'воистину', 'нашею', 'быть', 'никем', 'подобных', 'некоторая', 'го', 'свыше', 'него', 'ф', 'проще', 'несколько', 'некоторые', 'ела', 'казаться', 'которые', 'что', 'именно', 'может', 'пусть', 'сего', 'всея', 'ты', 'эту', 'у', 'это', 'своими', 'тобою', 'ежели', 'й', 'мое', 'некому', 'чьих', 'поскольку', 'своих', 'моем', 'нынешнее', 'отнелижа', 'вишь', 'причем', 'моим', 'однако', 'твой', 'мы', 'начала', 'ваше', 'мимо', 'всей', 'имел', 'потом', 'ура', 'ничто', 'коль', 'се', 'впрочем', 'могите', 'кого', 'аж', 'действительно', 'либо', 'имъ', 'наверняка', 'чьим', 'моею', 'ею', 'могу', 'каждое', 'нашей', 'эй', 'своё', 'твоя', 'можешь', 'оный', 'емъ', 'кабы', 'сам', 'более', 'насилу', 'нечему', 'нигде', 'какой', 'любых', 'покамест', 'свой', 'себе', 'моими', 'всячески', 'могло', 'томах', 'другая', 'нечем', 'посреди', 'некогда', 'итак', 'об', 'ч', 'подобным', 'наипаче', 'за', 'нельзя', 'н', 'хотела', 'сами', 'ау', 'негде', 'помимо', 'хочу', 'мой', 'самого', 'чаще', 'наперекор', 'ый', 'больше', 'иным', 'всех', 'твои', 'казался', 'зато', 'никого', 'моя', 'был', 'ест', 'откуда', 'нынешних', 'такое', 'вплоть', 'самой', 'благо', 'самых', 'его', 'любую', 'ая', 'ы', 'он', 'однажды', 'но', 'данная', 'будут', 'ради', 'мне', 'сверх', 'нынешней', 'скоро', 'такою', 'эти', 'бац', 'ниже', 'вместо', 'такая', 'моё', 'меж', 'многого', 'чхать', 'вновь', 'имеет', 'чей', 'ему', 'котором', 'этому', 'похожем', 'любыми', 'теми', 'ваша', 'имело', 'каждый', 'поди', 'незачем', 'рано', 'многое', 'понеже', 'якоже', 'одну', 'нашего', 'ш', 'них', 'само', 'никто', 'подобные', 'отсюда', 'дальше', 'никак', 'всего', 'м', 'вами', 'ые', 'вроде', 'наса', 'нее', 'той', 'необходимым', 'будто', 'ой', 'вполне', 'под', 'этак', 'тут', 'вперекор', 'ибо', 'р', 'вам', 'надо', 'внизу', 'данный', 'долго', 'необходимые', 'раньше', 'всею', 'б', 'безусловно', 'пока', 'давайте', 'никуда', 'этой', 'некоторый', 'иными', 'гав', 'часто', 'нужный', 'стал', 'нём', 'ними', 'ничего', 'вернее', 'наших', 'саму', 'хочешь', 'ух', 'чьё', 'рядом', 'фу', 'одному', 'про', 'таким', 'один', 'прям', 'они', 'давно', 'могла', 'почему', 'ю', 'будем', 'ниоткуда', 'те', 'куда', 'де', 'зря', 'еле'}\n",
            "Все все True True False\n",
            "счастливые счастливые True False False\n",
            "семьи семьи True False False\n",
            "похожи похожи True False False\n",
            "друг друг True False False\n",
            "на на True True False\n",
            "друга друга True False False\n",
            ", , False False True\n",
            "каждая каждая True True False\n",
            "несчастливая несчастливая True False False\n",
            "семья семья True False False\n",
            "несчастлива несчастлива True False False\n",
            "по по True True False\n",
            "- - False False True\n",
            "своему своему True True False\n",
            ". . False False True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPOAXAoV8Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5302af17-24a6-4213-e3d3-a7a04a79cf8a"
      },
      "source": [
        "try:\n",
        "    import stanfordnlp\n",
        "except ModuleNotFoundError:\n",
        "    !pip install stanfordnlp\n",
        "    import stanfordnlp\n",
        "\n",
        "stanfordnlp.download('ru')\n",
        "nlp = stanfordnlp.Pipeline(lang='ru')\n",
        "text = \"\"\"\n",
        "Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.\n",
        "\"\"\"\n",
        "doc = nlp(text)\n",
        "doc.sentences[0].print_dependencies()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m153.6/158.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (1.23.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.31.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->stanfordnlp) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->stanfordnlp) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordnlp) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n",
            "Using the default treebank \"ru_syntagrus\" for language \"ru\".\n",
            "Would you like to download the models for: ru_syntagrus now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: ru_syntagrus\n",
            "Download location: /root/stanfordnlp_resources/ru_syntagrus_models.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 236M/236M [00:44<00:00, 5.36MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/ru_syntagrus_models.zip\n",
            "Extracting models file for: ru_syntagrus\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus_tokenizer.pt', 'lang': 'ru', 'shorthand': 'ru_syntagrus', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus.pretrain.pt', 'lang': 'ru', 'shorthand': 'ru_syntagrus', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus_lemmatizer.pt', 'lang': 'ru', 'shorthand': 'ru_syntagrus', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/ru_syntagrus_models/ru_syntagrus.pretrain.pt', 'lang': 'ru', 'shorthand': 'ru_syntagrus', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "('Все', '3', 'det')\n",
            "('счастливые', '3', 'amod')\n",
            "('семьи', '4', 'nsubj')\n",
            "('похожи', '0', 'root')\n",
            "('друг', '4', 'obl')\n",
            "('на', '5', 'fixed')\n",
            "('друга', '5', 'fixed')\n",
            "(',', '12', 'punct')\n",
            "('каждая', '11', 'det')\n",
            "('несчастливая', '11', 'amod')\n",
            "('семья', '12', 'nsubj')\n",
            "('несчастлива', '4', 'conj')\n",
            "('по-своему', '12', 'advmod')\n",
            "('.', '12', 'punct')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stanfordnlp/models/depparse/model.py:157: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1772.)\n",
            "  unlabeled_scores.masked_fill_(diag, -float('inf'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHzUaQ2LY0Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea30504-5072-4d31-f837-0374f8f03be0"
      },
      "source": [
        "for s in doc.sentences:\n",
        "  for token in s.words:\n",
        "    #print(token)\n",
        "    print(token.text, token.lemma, token.upos, token.feats)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Все весь DET Case=Nom|Number=Plur\n",
            "счастливые счастливый ADJ Case=Nom|Degree=Pos|Number=Plur\n",
            "семьи семья NOUN Animacy=Inan|Case=Nom|Gender=Fem|Number=Plur\n",
            "похожи похожий ADJ Degree=Pos|Number=Plur|Variant=Short\n",
            "друг друг NOUN Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
            "на на ADP _\n",
            "друга друг NOUN Animacy=Anim|Case=Acc|Gender=Masc|Number=Sing\n",
            ", , PUNCT _\n",
            "каждая каждый DET Case=Nom|Gender=Fem|Number=Sing\n",
            "несчастливая несчастливый ADJ Case=Nom|Degree=Pos|Gender=Fem|Number=Sing\n",
            "семья семья NOUN Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
            "несчастлива несчастливый ADJ Degree=Pos|Gender=Fem|Number=Sing|Variant=Short\n",
            "по-своему по-своему ADV Degree=Pos\n",
            ". . PUNCT _\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIwHkGXWPZjy"
      },
      "source": [
        "### Морфологический анализ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXhbJ_olV8xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a0d011-11dc-4638-e3df-7c55ed042a26"
      },
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "import pprint\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "pprint.pprint(morph.parse('Ваня'))\n",
        "res = morph.parse('пила')\n",
        "print(len(res))\n",
        "pprint.pprint(res)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.8/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (0.7.2)\n",
            "[Parse(word='ваня', tag=OpencorporaTag('NOUN,anim,masc,Name sing,nomn'), normal_form='ваня', score=1.0, methods_stack=((DictionaryAnalyzer(), 'ваня', 424, 0),))]\n",
            "4\n",
            "[Parse(word='пила', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='пила', score=0.428571, methods_stack=((DictionaryAnalyzer(), 'пила', 55, 0),)),\n",
            " Parse(word='пила', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='пить', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'пила', 465, 8),)),\n",
            " Parse(word='пила', tag=OpencorporaTag('NOUN,anim,masc,Name sing,gent'), normal_form='пил', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'пила', 27, 1),)),\n",
            " Parse(word='пила', tag=OpencorporaTag('NOUN,anim,masc,Name sing,accs'), normal_form='пил', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'пила', 27, 3),))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(morph.parse('cтол'))\n",
        "pprint.pprint(morph.parse('cтола'))\n",
        "pprint.pprint(morph.parse('cтолу'))\n",
        "pprint.pprint(morph.parse('cтолом'))\n",
        "pprint.pprint(morph.parse('cтоле'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81t1mOyMNDZQ",
        "outputId": "bb025fff-999c-485e-b311-f89bf62a7045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parse(word='cтол', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='cтол', score=0.5, methods_stack=((DictionaryAnalyzer(), 'тол', 34, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтол', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='cтол', score=0.5, methods_stack=((DictionaryAnalyzer(), 'тол', 34, 3), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c')))]\n",
            "[Parse(word='cтола', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='cтол', score=0.5, methods_stack=((DictionaryAnalyzer(), 'тола', 34, 1), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтола', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,nomn'), normal_form='cтола', score=0.5, methods_stack=((DictionaryAnalyzer(), 'тола', 36, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c')))]\n",
            "[Parse(word='cтолу', tag=OpencorporaTag('NOUN,inan,masc sing,datv'), normal_form='cтол', score=0.5, methods_stack=((DictionaryAnalyzer(), 'толу', 34, 2), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтолу', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,accs'), normal_form='cтола', score=0.5, methods_stack=((DictionaryAnalyzer(), 'толу', 36, 3), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c')))]\n",
            "[Parse(word='cтолом', tag=OpencorporaTag('NOUN,inan,masc sing,ablt'), normal_form='cтол', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'толом', 34, 4), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтолом', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='cтолом', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'лом', 161, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'cто'))),\n",
            " Parse(word='cтолом', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='cтолом', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'лом', 161, 4), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'cто')))]\n",
            "[Parse(word='cтоле', tag=OpencorporaTag('NOUN,inan,masc sing,loct'), normal_form='cтол', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 34, 5), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,datv'), normal_form='cтола', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 36, 2), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,inan,femn,Sgtm,Geox sing,loct'), normal_form='cтола', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 36, 6), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,inan,masc sing,loct'), normal_form='cтоль', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 92, 5), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,anim,masc,Name sing,datv'), normal_form='cтоля', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 424, 2), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,anim,masc,Name sing,loct'), normal_form='cтоля', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'толе', 424, 6), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'c'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,anim,femn,Name sing,datv'), normal_form='cтоля', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'оле', 1939, 2), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'cт'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('NOUN,anim,femn,Name sing,loct'), normal_form='cтоля', score=0.12337662337662339, methods_stack=((DictionaryAnalyzer(), 'оле', 1939, 6), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'cт'))),\n",
            " Parse(word='cтоле', tag=OpencorporaTag('ADVB'), normal_form='cтоле', score=0.012987012987012988, methods_stack=((FakeDictionary(), 'cтоле', 3, 0), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'толе')))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJcbRcnKgLfZ"
      },
      "source": [
        "Тег — набор граммем (грамматических признаков), характеризующих данное слово.\n",
        "\n",
        "Например, тег 'VERB,impf, tran femn, sing,past,indc' означает, что слово — глагол (VERB) несовершенного вида (impf), переходный (tran), женского рода (femn), единственного числа (sing), прошедшего времени (past), изъявительного наклонения (indc).\n",
        "\n",
        "Полный список граммем (грамматических единиц) можно посмотреть [тут](http://opencorpora.org/dict.php?act=gram)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdlHZ4rjgvnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8954244-99be-4682-cee9-052798325d24"
      },
      "source": [
        "res = morph.parse('питона')[1]\n",
        "# В нулевом предположении будет родительный падеж - он чаще встречается\n",
        "# проверка на глагол\n",
        "print(\"VERB\" in res.tag)\n",
        "# проверка на винительный падеж\n",
        "print(\"accs\" in res.tag)\n",
        "# cуществительное в винительном падеже?\n",
        "print({\"NOUN\", 'accs'} in res.tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N94NQH8Kg29d"
      },
      "source": [
        "Кроме того, у каждого тега есть атрибуты, через которые можно получить часть речи, число и другие характеристики:\n",
        "\n",
        "Например, для глагола бывают теги:\n",
        "\n",
        "* p.tag.POS           # часть речи\n",
        "* p.tag.animacy       # одушевленность\n",
        "* p.tag.aspect        # вид: совершенный или несовершенный\n",
        "* p.tag.gender        # род (мужской, женский, средний)\n",
        "* p.tag.involvement   # включенность говорящего в действие\n",
        "* p.tag.mood          # наклонение (повелительное, изъявительное)\n",
        "* p.tag.number        # число (единственное, множественное)\n",
        "* p.tag.person        # лицо (1, 2, 3)\n",
        "* p.tag.tense         # время (настоящее, прошедшее, будущее)\n",
        "* p.tag.transitivity  # переходность (переходный, непереходный)\n",
        "* p.tag.voice         # залог (действительный, страдательный)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDTiUIP6hOXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a4fc32-dd15-43b3-f002-a70eafa9ed2d"
      },
      "source": [
        "word = morph.parse('случай')[0]\n",
        "print(word.inflect({'gent'}))\n",
        "print(word.inflect({'gent', 'plur'}))\n",
        "word = morph.parse('программировать')[0]\n",
        "print(word.inflect({'VERB', 'impf', 'plur', 'impr'}))\n",
        "print(word.inflect({'GRND'}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse(word='случая', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='случай', score=1.0, methods_stack=((DictionaryAnalyzer(), 'случая', 180, 1),))\n",
            "Parse(word='случаев', tag=OpencorporaTag('NOUN,inan,masc plur,gent'), normal_form='случай', score=1.0, methods_stack=((DictionaryAnalyzer(), 'случаев', 180, 7),))\n",
            "Parse(word='программируйте', tag=OpencorporaTag('VERB,impf,tran plur,impr,excl'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируйте', 171, 12),))\n",
            "Parse(word='программируя', tag=OpencorporaTag('GRND,impf,tran pres'), normal_form='программировать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'программируя', 171, 121),))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJRXIlihg4i"
      },
      "source": [
        "Нормальную (начальную) форму слова можно получить через атрибуты Parse.normal_form и Parse.normalized. Например, для глаголов в нем будет храниться инфинитив. Таким образом, можно привести любую форму глагола к единому виду.\n",
        "\n",
        "Но что считается за нормальную форму у других слов? Например, возьмем слово «изучающим». Иногда мы захотим нормализовать его в «изучать», иногда — в «изучающий», иногда — в «изучающая».\n",
        "\n",
        "Посмотрим на примере, что сделает pymorphy2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzpVudlRhdWQ"
      },
      "source": [
        "morph.parse('изучающим')[0].normal_form\n",
        "# => 'изучать'\n",
        "morph.parse('изучающим')[0].inflect({'sing', 'nomn'}).word\n",
        "# => 'изучающий'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}